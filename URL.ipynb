{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f36882-e532-4b81-b181-3cc743a75a66",
   "metadata": {},
   "source": [
    "#### Importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7ebfc3f-ea90-4e4c-99a3-8de2093943c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8543b1-af2f-49b2-97c2-e143abc8f395",
   "metadata": {},
   "source": [
    "#### Importing the Dataset\n",
    "#### Source :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af857b1b-828a-466a-9ce2-a728d67f7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'Dataset/data_url.csv'\n",
    "url_csv = pd.read_csv(url)\n",
    "\n",
    "#converting the data from csv to dataframe for easy handling\n",
    "url_df = pd.DataFrame(url_csv)\n",
    "\n",
    "#to convert into array \n",
    "url_df = np.array(url_df)  \n",
    "random.shuffle(url_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5371e05-8d10-4b3e-81e4-5ba341dc9a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['diaryofagameaddict.com', 'bad'],\n",
       "       ['diaryofagameaddict.com', 'bad'],\n",
       "       ['iamagameaddict.com', 'bad'],\n",
       "       ['kalantzis.net', 'bad'],\n",
       "       ['iamagameaddict.com', 'bad'],\n",
       "       ['espdesign.com.au', 'bad'],\n",
       "       ['diaryofagameaddict.com', 'bad'],\n",
       "       ['iamagameaddict.com', 'bad'],\n",
       "       ['kalantzis.net', 'bad'],\n",
       "       ['tubemoviez.com', 'bad']], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_df[0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c3401-eece-4447-b97a-df58bfbb2324",
   "metadata": {},
   "source": [
    "#### Seperating the data according to it's characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cbd7a0e-2b13-4acc-971a-c0aa0feb88a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d[1] for d in url_df]                 # 'y' holds the category whether url is good or bad.\n",
    "urls = [d[0] for d in url_df]              # 'urls' holds the actual url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da635a99-8b28-42a8-9178-995ba9f803fc",
   "metadata": {},
   "source": [
    "#### Since the urls are different from normal text, we need to use a sanitization method to get the relevant data from raw urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c843fb-22c6-41a7-81df-d6a3c74e2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitization(web):\n",
    "    web = web.lower()\n",
    "    token = []\n",
    "    dot_token_slash = []\n",
    "    raw_slash = str(web).split('/') # Removes '/' in the url wherever encountered and stores the rest into a list. \n",
    "    for i in raw_slash:\n",
    "        # removing slash to get token\n",
    "        raw1 = str(i).split('-')                   # Removing '-' in the remaining url and stores the rest into a list.\n",
    "        slash_token = []\n",
    "        for j in range(0,len(raw1)):\n",
    "            # removing dot to get the tokens\n",
    "            raw2 = str(raw1[j]).split('.')         # Removing '.' in the remaining url and stores the rest into a list.\n",
    "            slash_token = slash_token + raw2\n",
    "            dot_token_slash = dot_token_slash + raw1 + slash_token       # Stores the final string after removing all the \n",
    "    # to remove same words\n",
    "    token = list(set(dot_token_slash))  \n",
    "    if 'com' in token:\n",
    "        #remove com\n",
    "        token.remove('com')\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc84417-a539-4e1b-a4eb-25d111e74a1e",
   "metadata": {},
   "source": [
    "#### We will have to pass the data to our custom vectorizer function using Tf-idf approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f054c6cc-d274-4f67-a1aa-c3d7f3ee0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# term-frequency and inverse-document-frequency\n",
    "# TF gives us information on how often a term appears in a document and IDF gives us information about the relative rarity of a term in the collection of documents.\n",
    "vectorizer = TfidfVectorizer(tokenizer=sanitization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2367b-dbcc-4442-b5c8-c5351419c30b",
   "metadata": {},
   "source": [
    "#### Splitting the test set and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e416a259-94a0-4d50-a9ff-b2bcfc216e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.fit_transform(urls)       # will standardize the urls by converting them to numbers.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d4378f-10c6-47ad-b908-7217e7d7ad7d",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b850df95-ee23-4755-9a66-e74f4ffcac2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 98.53 %\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression(solver='lbfgs', max_iter=1000)                  # Logistic regression (Binary)\n",
    "lgr.fit(x_train, y_train)\n",
    "score = lgr.score(x_test, y_test)\n",
    "vectorizer_save = vectorizer\n",
    "print(\"score: {0:.2f} %\".format(100 * score)) # Got a score of 98.53 % ; a very good score which terms our model as realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c0b181-99f9-43e0-b0c2-417ad6e77445",
   "metadata": {},
   "source": [
    "#### Saving the model and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540609e3-93d6-4389-9f19-8c7f7630c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"pickel_model.pkl\"\n",
    "with open(file, 'wb') as f:\n",
    "    pickle.dump(lgr, f)\n",
    "f.close()\n",
    "\n",
    "file2 = \"pickel_vector.pkl\"\n",
    "with open(file2,'wb') as f2:\n",
    "    pickle.dump(vectorizer_save, f2)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ca561-635d-4c5e-a3ea-ca8f1aea60e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
